package downloader

import (
	"bytes"
	"context"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"math/rand"
	"net/http"
	"net/url"
	"os"
	"os/signal"
	"path/filepath"
	"regexp"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"golang.org/x/net/html"
)

const (
	DefaultWorkers     = 6
	DefaultMaxDepth    = 30
	DefaultRetries     = 5
	DefaultDelay       = 2 * time.Second
	DefaultMaxFileSize = 15 << 20
	DefaultUserAgent   = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
	StateFileExtension = ".state.json"
)

var (
	ErrInvalidURL     = errors.New("invalid URL")
	ErrDownloadFailed = errors.New("download failed after retries")
	ErrParseFailed    = errors.New("parsing failed")
)

type FileMetadata struct {
	URL         string
	ContentType string
	Hash        string
	Depth       int
}

type JobStats struct {
	TotalFiles      int64
	DownloadedBytes int64
	Failed          int64
	Skipped         int64
	Speed           float64
	ETA             time.Duration
	FileTypes       map[string]int64
	StartTime       time.Time
}

type JobState struct {
	ID          string
	RootURL     string
	PendingURLs []string
	DepthMap    map[string]int
	Stats       JobStats
	Config      Config
}

type Config struct {
	Workers     int
	MaxDepth    int
	Retries     int
	Delay       time.Duration
	MaxFileSize int64
	OutputDir   string
	UserAgent   string
}

type ContentParser interface {
	CanParse(contentType string) bool
	Parse(content []byte, baseURL string) ([]string, error)
}

type URLFilter interface {
	ShouldDownload(url string) bool
	FilterReason(url string) string
}

type ContentHandler interface {
	Priority() int
	Handle(content []byte, meta FileMetadata) ([]byte, error)
}

// HTMLParser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –°–´–†–´–• —Å—Å—ã–ª–æ–∫ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
type HTMLParser struct{}

func (p *HTMLParser) CanParse(ct string) bool { return strings.Contains(ct, "text/html") }

func (p *HTMLParser) Parse(content []byte, baseURL string) ([]string, error) {
	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return nil, ErrParseFailed
	}
	var links []string
	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			switch n.Data {
			case "a", "link":
				for _, a := range n.Attr {
					if a.Key == "href" {
						links = append(links, a.Val)
					}
				}
			case "img", "script", "source":
				for _, a := range n.Attr {
					if a.Key == "src" {
						links = append(links, a.Val)
					}
				}
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)

	// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –°–´–†–´–ï —Å—Å—ã–ª–∫–∏ (–±–µ–∑ –∑–∞–º–µ–Ω—ã .php ‚Üí .html)
	return resolveRawLinks(links, baseURL), nil
}

type CSSParser struct{}

func (p *CSSParser) CanParse(ct string) bool { return strings.Contains(ct, "text/css") }

func (p *CSSParser) Parse(content []byte, baseURL string) ([]string, error) {
	re := regexp.MustCompile(`(?i)url\s*\(\s*['"]?([^'")]+)['"]?\s*\)`)
	matches := re.FindAllSubmatch(content, -1)
	var links []string
	for _, m := range matches {
		if len(m[1]) > 0 {
			links = append(links, string(m[1]))
		}
	}
	return resolveRawLinks(links, baseURL), nil
}

// resolveRawLinks ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –ë–ï–ó –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
func resolveRawLinks(links []string, baseURL string) []string {
	var resolved []string
	base, _ := url.Parse(baseURL)
	bad := []string{"devnull", "410011174743222", "yoomoney", "t.me/metanitcom"}

	for _, l := range links {
		l = strings.TrimSpace(l)
		if strings.HasPrefix(l, "data:") || strings.HasPrefix(l, "#") || strings.HasPrefix(l, "javascript:") {
			continue
		}
		// Handle protocol-relative URLs
		if strings.HasPrefix(l, "//") {
			l = "https:" + l
		}
		u, err := url.Parse(l)
		if err != nil {
			continue
		}
		res := base.ResolveReference(u).String()

		skip := false
		for _, p := range bad {
			if strings.Contains(res, p) {
				skip = true
				break
			}
		}
		if !skip {
			resolved = append(resolved, res)
			log.Printf("Resolved RAW link: %s", res)
		}
	}
	return resolved
}

func replacePhpToHtmlLinks(content []byte, baseURL string) ([]byte, error) {
	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return content, nil
	}

	baseParsed, _ := url.Parse(baseURL)

	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			for i := range n.Attr {
				attr := &n.Attr[i]
				if attr.Key == "href" || attr.Key == "src" || attr.Key == "action" {
					orig := attr.Val

					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
					if strings.HasPrefix(orig, "data:") ||
						strings.HasPrefix(orig, "#") ||
						strings.HasPrefix(orig, "javascript:") ||
						strings.HasPrefix(orig, "mailto:") ||
						strings.HasPrefix(orig, "tel:") {
						continue
					}

					// –†–∞–∑–±–∏—Ä–∞–µ–º URL
					u, err := url.Parse(orig)
					if err != nil {
						continue
					}

					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
					if u.Host != "" && u.Host != baseParsed.Host {
						continue
					}

					// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—É—Ç—å
					path := u.Path

					// –ï—Å–ª–∏ –ø—É—Ç—å –ø—É—Å—Ç–æ–π –∏–ª–∏ –∫–æ—Ä–Ω–µ–≤–æ–π
					if path == "" || path == "/" {
						// –î–ª—è –∫–æ—Ä–Ω—è –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
						u.Path = "/"
						attr.Val = u.String()
						continue
					}

					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ—Å—É—Ä—Å—ã (CSS, JS, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
					// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
					lowerPath := strings.ToLower(path)
					isResource := strings.HasSuffix(lowerPath, ".css") ||
						strings.HasSuffix(lowerPath, ".js") ||
						strings.HasSuffix(lowerPath, ".mjs") ||
						strings.HasSuffix(lowerPath, ".cjs") ||
						strings.HasSuffix(lowerPath, ".png") ||
						strings.HasSuffix(lowerPath, ".jpg") ||
						strings.HasSuffix(lowerPath, ".jpeg") ||
						strings.HasSuffix(lowerPath, ".gif") ||
						strings.HasSuffix(lowerPath, ".svg") ||
						strings.HasSuffix(lowerPath, ".ico") ||
						strings.HasSuffix(lowerPath, ".woff") ||
						strings.HasSuffix(lowerPath, ".woff2") ||
						strings.HasSuffix(lowerPath, ".ttf") ||
						strings.HasSuffix(lowerPath, ".eot") ||
						strings.HasSuffix(lowerPath, ".otf") ||
						strings.HasSuffix(lowerPath, ".mp4") ||
						strings.HasSuffix(lowerPath, ".webm") ||
						strings.HasSuffix(lowerPath, ".mp3") ||
						strings.HasSuffix(lowerPath, ".wav") ||
						strings.HasSuffix(lowerPath, ".ogg")

					if isResource {
						// –î–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –∫–∞–∫ –µ—Å—Ç—å
						continue
					}

					// –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º .php —Å—Å—ã–ª–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü)
					if strings.HasSuffix(lowerPath, ".php") {
						// –£–±–∏—Ä–∞–µ–º .php
						newPath := strings.TrimSuffix(path, ".php")

						// –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª "index.php", –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ "/"
						if strings.HasSuffix(strings.ToLower(newPath), "/index") {
							newPath = strings.TrimSuffix(newPath, "/index")
							if newPath == "" {
								newPath = "/"
							}
						} else if strings.EqualFold(newPath, "index") {
							// –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ "index.php", —Ç–æ–∂–µ –≤ "/"
							newPath = "/"
						}

						u.Path = newPath
						attr.Val = u.String()
						log.Printf("üîó Rewrote PHP link: %s ‚Üí %s", orig, attr.Val)
					} else if strings.HasSuffix(lowerPath, ".html") ||
						strings.HasSuffix(lowerPath, ".htm") {
						// –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º .html —Å—Å—ã–ª–∫–∏
						// –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
						newPath := strings.TrimSuffix(
							strings.TrimSuffix(path, ".html"), ".htm")

						// –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª "index.html", –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ "/"
						if strings.HasSuffix(strings.ToLower(newPath), "/index") {
							newPath = strings.TrimSuffix(newPath, "/index")
							if newPath == "" {
								newPath = "/"
							}
						} else if strings.EqualFold(newPath, "index") {
							// –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ "index.html", —Ç–æ–∂–µ –≤ "/"
							newPath = "/"
						}

						u.Path = newPath
						attr.Val = u.String()
						log.Printf("üîó Rewrote HTML link: %s ‚Üí %s", orig, attr.Val)
					}
				}
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)

	var buf bytes.Buffer
	html.Render(&buf, doc)
	return buf.Bytes(), nil
}

// FileSaveStrategy - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
type FileSaveStrategy interface {
	ShouldSaveAsDirectory(url string, contentType string) bool
	GetSavePath(outputDir string, url string, contentType string) (string, string) // –ø—É—Ç—å, –∏–º—è —Ñ–∞–π–ª–∞
	RewriteLink(originalURL, baseURL string) string
}

// DirectoryIndexStrategy - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è "–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å index.html"
type DirectoryIndexStrategy struct{}

func (s *DirectoryIndexStrategy) ShouldSaveAsDirectory(urlStr string, contentType string) bool {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		return false
	}
	path := parsed.Path

	// –î–ª—è HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
	if strings.Contains(contentType, "text/html") {
		return true
	}

	// –î–ª—è .php —Ñ–∞–π–ª–æ–≤ —Ç–æ–∂–µ (–¥–∞–∂–µ –µ—Å–ª–∏ content-type –Ω–µ —É–∫–∞–∑–∞–Ω)
	if strings.HasSuffix(strings.ToLower(path), ".php") {
		return true
	}

	// –î–ª—è –ø—É—Ç–µ–π –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ä–µ—Å—É—Ä—Å–∞–º–∏
	if !strings.Contains(path, ".") && path != "/" && path != "" {
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ API endpoint –∏–ª–∏ –ø–æ–¥–æ–±–Ω—ã–º
		if !strings.Contains(path, "/api/") &&
			!strings.Contains(path, "/ajax/") &&
			!strings.Contains(path, "/rest/") {
			return true
		}
	}

	return false
}

func (s *DirectoryIndexStrategy) GetSavePath(outputDir string, urlStr string, contentType string) (string, string) {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		log.Printf("Parse error in GetSavePath: %v", err)
		return "", ""
	}
	host := parsed.Host
	path := parsed.Path

	// –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
	if path == "" || path == "/" {
		path = "/"
	}

	cleanPath := strings.TrimPrefix(path, "/")
	if cleanPath == "" {
		cleanPath = "index"
	}

	// –†–∞–∑–¥–µ–ª—è–µ–º –ø—É—Ç—å –Ω–∞ —á–∞—Å—Ç–∏
	var parts []string
	if cleanPath != "" {
		parts = strings.Split(cleanPath, "/")
	}

	// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
	if len(parts) > 0 {
		lastPart := parts[len(parts)-1]

		// –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü
		lowerLast := strings.ToLower(lastPart)
		if strings.HasSuffix(lowerLast, ".php") ||
			strings.HasSuffix(lowerLast, ".html") ||
			strings.HasSuffix(lowerLast, ".htm") ||
			strings.HasSuffix(lowerLast, ".asp") ||
			strings.HasSuffix(lowerLast, ".aspx") ||
			strings.HasSuffix(lowerLast, ".jsp") {

			// –£–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
			ext := filepath.Ext(lastPart)
			newName := strings.TrimSuffix(lastPart, ext)

			if newName == "" || strings.EqualFold(newName, "index") {
				parts = parts[:len(parts)-1]
			} else {
				parts[len(parts)-1] = newName
			}
		} else if strings.EqualFold(lastPart, "index") {
			parts = parts[:len(parts)-1]
		}
	}

	// –°—Ç—Ä–æ–∏–º –ø—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
	basePath := filepath.Join(outputDir, host)

	var saveDir string
	if len(parts) > 0 {
		saveDir = filepath.Join(append([]string{basePath}, parts...)...)
	} else {
		saveDir = basePath
	}

	return saveDir, "index.html"
}

func (s *DirectoryIndexStrategy) RewriteLink(originalURL, baseURL string) string {
	parsed, err1 := url.Parse(originalURL)
	baseParsed, err2 := url.Parse(baseURL)

	if err1 != nil || err2 != nil {
		return originalURL
	}

	// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
	if parsed.Host != "" && parsed.Host != baseParsed.Host {
		return originalURL
	}

	if strings.HasPrefix(originalURL, "#") ||
		strings.HasPrefix(originalURL, "javascript:") ||
		strings.HasPrefix(originalURL, "mailto:") ||
		strings.HasPrefix(originalURL, "tel:") ||
		strings.HasPrefix(originalURL, "data:") {
		return originalURL
	}

	// –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏
	sourcePath := baseParsed.Path
	targetPath := parsed.Path

	// –ï—Å–ª–∏ –ø—É—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∏–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø—É—Ç—å –ø—É—Å—Ç–æ–π
	if targetPath == "" || targetPath == "/" {
		parsed.Path = "/"
		return parsed.String()
	}

	// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏
	if !strings.HasPrefix(targetPath, "/") {
		// –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
		return originalURL
	}

	// –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
	lowerTarget := strings.ToLower(targetPath)
	pageExtensions := []string{".php", ".html", ".htm", ".asp", ".aspx", ".jsp"}

	for _, ext := range pageExtensions {
		if strings.HasSuffix(lowerTarget, ext) {
			// –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
			newPath := strings.TrimSuffix(targetPath, ext)

			// –û–±—Ä–∞–±–æ—Ç–∫–∞ index —Å—Ç—Ä–∞–Ω–∏—Ü
			if strings.HasSuffix(strings.ToLower(newPath), "/index") {
				newPath = strings.TrimSuffix(newPath, "/index")
			} else if strings.EqualFold(newPath, "index") {
				newPath = "/"
			}

			if newPath == "" {
				newPath = "/"
			}

			// –¢–µ–ø–µ—Ä—å –≤—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç sourcePath –∫ newPath
			if sourcePath != "/" && newPath != "/" {
				relativePath := calculateRelativePath(sourcePath, newPath)
				if relativePath != "" {
					parsed.Path = relativePath
					return parsed.String()
				}
			}

			parsed.Path = newPath
			return parsed.String()
		}
	}

	// –î–ª—è –ø—É—Ç–µ–π –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ç–æ–∂–µ –≤—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
	if !strings.Contains(targetPath, ".") {
		relativePath := calculateRelativePath(sourcePath, targetPath)
		if relativePath != "" && relativePath != targetPath {
			parsed.Path = relativePath
			return parsed.String()
		}
	}

	return originalURL
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
func calculateRelativePath(fromPath, toPath string) string {
	// –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç–∏
	if fromPath == "" || fromPath == "/" {
		fromPath = "/"
	} else if !strings.HasSuffix(fromPath, "/") {
		// –ï—Å–ª–∏ fromPath –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ /, –±–µ—Ä–µ–º –µ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
		fromPath = filepath.Dir(fromPath)
		if fromPath == "." {
			fromPath = "/"
		} else {
			fromPath = fromPath + "/"
		}
	}

	if toPath == "" || toPath == "/" {
		toPath = "/"
	} else if !strings.HasSuffix(toPath, "/") {
		toPath = toPath + "/"
	}

	// –†–∞–∑–±–∏–≤–∞–µ–º –ø—É—Ç–∏ –Ω–∞ —á–∞—Å—Ç–∏
	fromParts := strings.Split(strings.Trim(fromPath, "/"), "/")
	toParts := strings.Split(strings.Trim(toPath, "/"), "/")

	// –ù–∞—Ö–æ–¥–∏–º –æ–±—â—É—é —á–∞—Å—Ç—å
	common := 0
	for i := 0; i < len(fromParts) && i < len(toParts); i++ {
		if fromParts[i] == toParts[i] {
			common++
		} else {
			break
		}
	}

	// –°—Ç—Ä–æ–∏–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
	var result strings.Builder

	// –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞–≤–µ—Ä—Ö –∏–∑ fromPath
	for i := common; i < len(fromParts); i++ {
		if result.Len() > 0 {
			result.WriteString("/")
		}
		result.WriteString("..")
	}

	// –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à—É—é—Å—è —á–∞—Å—Ç—å toPath
	for i := common; i < len(toParts); i++ {
		if result.Len() > 0 {
			result.WriteString("/")
		}
		result.WriteString(toParts[i])
	}

	if result.Len() == 0 {
		return "./"
	}

	return result.String()
}

// FileOnlyStrategy - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è "–ø—Ä–æ—Å—Ç–æ —Ñ–∞–π–ª" –¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤
type FileOnlyStrategy struct{}

func (s *FileOnlyStrategy) ShouldSaveAsDirectory(urlStr string, contentType string) bool {
	// –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ñ–∞–π–ª
	return false
}

func (s *FileOnlyStrategy) GetSavePath(outputDir string, urlStr string, contentType string) (string, string) {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		log.Printf("Parse error in FileOnlyStrategy: %v", err)
		return "", ""
	}
	host := parsed.Host
	path := parsed.Path

	if path == "" || path == "/" {
		path = "/index.html"
	}

	cleanPath := strings.TrimPrefix(path, "/")
	if cleanPath == "" {
		cleanPath = "index.html"
	}

	saveDir := filepath.Join(outputDir, host, filepath.Dir(cleanPath))
	fileName := filepath.Base(cleanPath)

	return saveDir, fileName
}

func (s *FileOnlyStrategy) RewriteLink(originalURL, baseURL string) string {
	// –î–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
	return originalURL
}

// StrategyAnalyzer - –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
type StrategyAnalyzer struct {
	strategies []FileSaveStrategy
}

func NewStrategyAnalyzer() *StrategyAnalyzer {
	return &StrategyAnalyzer{
		strategies: []FileSaveStrategy{
			&DirectoryIndexStrategy{},
			&FileOnlyStrategy{},
		},
	}
}

func (a *StrategyAnalyzer) Analyze(urlStr string, contentType string, content []byte) FileSaveStrategy {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		// –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å URL, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ñ–∞–π–ª–æ–≤
		return &FileOnlyStrategy{}
	}

	path := parsed.Path

	// –ê–Ω–∞–ª–∏–∑ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
	lowerPath := strings.ToLower(path)

	// –†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ñ–∞–π–ª—ã)
	resourceExtensions := []string{
		".css", ".js", ".mjs", ".cjs",
		".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp",
		".woff", ".woff2", ".ttf", ".eot", ".otf",
		".mp4", ".webm", ".mp3", ".wav", ".ogg", ".avi", ".mov",
		".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
		".zip", ".rar", ".7z", ".tar", ".gz",
		".json", ".xml", ".txt", ".csv",
	}

	for _, ext := range resourceExtensions {
		if strings.HasSuffix(lowerPath, ext) {
			return &FileOnlyStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type
	if contentType != "" {
		// –†–µ—Å—É—Ä—Å–Ω—ã–µ Content-Type
		resourceContentTypes := []string{
			"text/css",
			"application/javascript", "application/x-javascript",
			"image/", "font/", "audio/", "video/",
			"application/pdf", "application/zip",
			"application/json", "application/xml",
		}

		for _, ct := range resourceContentTypes {
			if strings.Contains(contentType, ct) {
				return &FileOnlyStrategy{}
			}
		}

		// HTML Content-Type
		if strings.Contains(contentType, "text/html") {
			return &DirectoryIndexStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 3: –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–µ—Å–ª–∏ Content-Type –Ω–µ —É–∫–∞–∑–∞–Ω)
	if contentType == "" || contentType == "application/octet-stream" {
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –±–∞–π—Ç—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ HTML —Ç–µ–≥–æ–≤
		contentStr := string(content)
		if len(contentStr) > 100 {
			sample := strings.ToLower(contentStr[:100])
			if strings.Contains(sample, "<!doctype") ||
				strings.Contains(sample, "<html") ||
				strings.Contains(sample, "<head") ||
				strings.Contains(sample, "<body") {
				return &DirectoryIndexStrategy{}
			}
		}

		// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤ URL
		if strings.HasSuffix(lowerPath, ".php") ||
			strings.HasSuffix(lowerPath, ".html") ||
			strings.HasSuffix(lowerPath, ".htm") ||
			strings.HasSuffix(lowerPath, ".asp") ||
			strings.HasSuffix(lowerPath, ".aspx") ||
			strings.HasSuffix(lowerPath, ".jsp") {
			return &DirectoryIndexStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 4: –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—É—Ç–µ–π
	// –ï—Å–ª–∏ –ø—É—Ç—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–∏–ø–∏—á–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
	staticPatterns := []string{
		"/static/", "/assets/", "/public/", "/resources/",
		"/css/", "/js/", "/images/", "/img/", "/fonts/",
		"/uploads/", "/media/", "/downloads/",
	}

	for _, pattern := range staticPatterns {
		if strings.Contains(path, pattern) {
			return &FileOnlyStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 5: –ü—É—Ç–∏ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
	if !strings.Contains(path, ".") && path != "/" && path != "" {
		// –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–∏–±–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è), –ª–∏–±–æ API endpoint
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã API
		apiPatterns := []string{"/api/", "/ajax/", "/rest/", "/graphql", "/auth/"}
		for _, pattern := range apiPatterns {
			if strings.Contains(path, pattern) {
				return &FileOnlyStrategy{}
			}
		}

		// –ï—Å–ª–∏ –Ω–µ API, —Ç–æ —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
		return &DirectoryIndexStrategy{}
	}

	// –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
	return &DirectoryIndexStrategy{}
}

type DefaultURLFilter struct {
	domain   string
	basePath string
}

func (f *DefaultURLFilter) ShouldDownload(u string) bool {
	parsed, err := url.Parse(u)
	if err != nil {
		return false
	}
	if parsed.Host != f.domain {
		return false
	}

	pathNoQ := strings.Split(parsed.Path, "?")[0]

	// –†–∞–∑—Ä–µ—à–∞–µ–º —Ñ–∞–π–ª—ã –≤–Ω—É—Ç—Ä–∏ BasePath
	if strings.HasPrefix(pathNoQ, f.basePath) {
		return true
	}

	// –†–∞–∑—Ä–µ—à–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã –∏–∑ –ª—é–±—ã—Ö –ø—É—Ç–µ–π
	exts := []string{".css", ".js", ".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".woff", ".ttf", ".webp", ".woff2"}
	for _, e := range exts {
		if strings.HasSuffix(strings.ToLower(parsed.Path), e) {
			return true
		}
	}

	// –†–∞–∑—Ä–µ—à–∞–µ–º .php —Ñ–∞–π–ª—ã –∏–∑ –ª—é–±—ã—Ö –ø—É—Ç–µ–π (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è)
	if strings.HasSuffix(strings.ToLower(parsed.Path), ".php") {
		return true
	}

	return false
}

func (f *DefaultURLFilter) FilterReason(u string) string {
	return "outside base path or not asset"
}

type LinkRewriterHandlerV2 struct {
	outputDir string
	analyzer  *StrategyAnalyzer
}

func (h *LinkRewriterHandlerV2) Priority() int { return 10 }

func (h *LinkRewriterHandlerV2) Handle(content []byte, meta FileMetadata) ([]byte, error) {
	// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-HTML –∫–æ–Ω—Ç–µ–Ω—Ç
	if !strings.Contains(meta.ContentType, "text/html") {
		return content, nil
	}

	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return content, nil
	}

	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			for i := range n.Attr {
				attr := &n.Attr[i]
				if attr.Key == "href" || attr.Key == "src" || attr.Key == "action" {
					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏
					if attr.Val == "" {
						continue
					}

					// –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (file://) –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
					if strings.HasPrefix(attr.Val, "file://") {
						continue
					}

					// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –∏ –≤—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
					strategy := h.analyzer.Analyze(attr.Val, "", nil)
					// –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Å—Å—ã–ª–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
					newURL := strategy.RewriteLink(attr.Val, meta.URL)

					if newURL != attr.Val {
						// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
						if !strings.Contains(newURL, "://") && !strings.HasPrefix(newURL, "/") {
							// –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ
							if strings.HasPrefix(newURL, "./") || strings.HasPrefix(newURL, "../") {
								attr.Val = newURL
							} else {
								// –î–æ–±–∞–≤–ª—è–µ–º ./ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
								attr.Val = "./" + newURL
							}
						} else {
							attr.Val = newURL
						}
						log.Printf("üîó Rewrote link: %s ‚Üí %s (from: %s)", attr.Val, newURL, meta.URL)
					}
				}
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)

	var buf bytes.Buffer
	html.Render(&buf, doc)
	return buf.Bytes(), nil
}

// SaveFileV2 - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
func SaveFileV2(outputDir, originalURL string, content []byte, ct string) (string, error) {
	analyzer := NewStrategyAnalyzer()
	strategy := analyzer.Analyze(originalURL, ct, content)

	// –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∏ –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
	saveDir, fileName := strategy.GetSavePath(outputDir, originalURL, ct)
	if saveDir == "" || fileName == "" {
		return "", fmt.Errorf("failed to get save path for %s", originalURL)
	}

	// –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
	if err := os.MkdirAll(saveDir, 0755); err != nil {
		log.Printf("Mkdir error %s: %v", saveDir, err)
		return "", err
	}

	// –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
	fullPath := filepath.Join(saveDir, fileName)

	// –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
	if err := ioutil.WriteFile(fullPath, content, 0644); err != nil {
		log.Printf("Write error %s: %v", fullPath, err)
		return "", err
	}

	log.Printf("‚úÖ Saved [%T]: %s ‚Üí %s", strategy, originalURL, fullPath)
	return fullPath, nil
}
func NormalizeURL(u string) (string, error) {
	pu, err := url.Parse(u)
	if err != nil {
		return "", err
	}

	pu.Fragment = ""

	path := pu.Path
	if path == "" {
		path = "/"
	}

	// Normalize index.html/index.htm paths
	lower := strings.ToLower(path)
	if strings.HasSuffix(lower, "/index.html") || strings.HasSuffix(lower, "/index.htm") {
		path = strings.TrimSuffix(path, "/index.html")
		path = strings.TrimSuffix(path, "/index.htm")
		if path == "" {
			path = "/"
		}
	} else if strings.HasSuffix(lower, "index.html") || strings.HasSuffix(lower, "index.htm") {
		path = strings.TrimSuffix(path, "index.html")
		path = strings.TrimSuffix(path, "index.htm")
		if path == "" {
			path = "/"
		}
	}

	pu.Path = path

	result := pu.String()
	log.Printf("üîó NormalizeURL: %s ‚Üí %s", u, result)
	return result, nil
}

func ContentHash(b []byte) string {
	h := sha256.Sum256(b)
	return hex.EncodeToString(h[:])
}

type Downloader struct {
	client    *http.Client
	retries   int
	delay     time.Duration
	maxSize   int64
	userAgent string
}

func NewDownloader(c Config) *Downloader {
	return &Downloader{
		client: &http.Client{
			Transport: &http.Transport{
				MaxIdleConns:    c.Workers * 2,
				IdleConnTimeout: 30 * time.Second,
			},
			CheckRedirect: func(r *http.Request, v []*http.Request) error {
				log.Printf("Redirect: %s ‚Üí %s", v[len(v)-1].URL, r.URL)
				return nil
			},
			Timeout: 30 * time.Second,
		},
		retries:   c.Retries,
		delay:     c.Delay,
		maxSize:   c.MaxFileSize,
		userAgent: c.UserAgent,
	}
}

func (d *Downloader) Download(ctx context.Context, u string) ([]byte, string, error) {
	log.Printf("DOWNLOAD REQUEST: %s", u)

	for attempt := 1; attempt <= d.retries; attempt++ {
		req, err := http.NewRequestWithContext(ctx, "GET", u, nil)
		if err != nil {
			log.Printf("Request creation error for %s: %v", u, err)
			return nil, "", err
		}

		req.Header.Set("User-Agent", d.userAgent)
		req.Header.Set("Referer", "https://metanit.com/")
		req.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
		req.Header.Set("Accept-Language", "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7")

		resp, err := d.client.Do(req)
		if err != nil {
			log.Printf("HTTP error for %s (attempt %d): %v", u, attempt, err)
			if attempt == d.retries {
				return nil, "", ErrDownloadFailed
			}
			time.Sleep(d.delay + time.Duration(rand.Intn(1000))*time.Millisecond)
			continue
		}

		log.Printf("RESPONSE: %s ‚Üí %d %s", u, resp.StatusCode, resp.Header.Get("Content-Type"))

		if resp.StatusCode != 200 {
			resp.Body.Close()
			if resp.StatusCode == 404 {
				log.Printf("‚ùå 404 Not Found: %s", u)
				return nil, "", fmt.Errorf("404 Not Found: %s", u)
			}
			log.Printf("HTTP error status %d for %s (attempt %d)", resp.StatusCode, u, attempt)

			if attempt == d.retries {
				return nil, "", fmt.Errorf("status %d", resp.StatusCode)
			}
			time.Sleep(d.delay + time.Duration(rand.Intn(1000))*time.Millisecond)
			continue
		}

		content, err := io.ReadAll(io.LimitReader(resp.Body, d.maxSize+1))
		resp.Body.Close()

		if err != nil {
			log.Printf("Read error for %s: %v", u, err)
			return nil, "", err
		}

		if len(content) > int(d.maxSize) {
			log.Printf("File too large: %s (%d bytes)", u, len(content))
			return nil, "", errors.New("file too large")
		}

		log.Printf("SUCCESS: Downloaded %s (%d bytes)", u, len(content))
		return content, resp.Header.Get("Content-Type"), nil
	}

	return nil, "", ErrDownloadFailed
}

type Job struct {
	ID         string
	RootURL    string
	Config     Config
	Filter     URLFilter
	Parsers    []ContentParser
	Handlers   []ContentHandler
	Downloader *Downloader
	BasePath   string

	mu           sync.Mutex
	pending      chan string
	visited      map[string]bool
	hashes       map[string]bool
	depths       map[string]int
	stats        JobStats
	ctx          context.Context
	cancel       context.CancelFunc
	wg           sync.WaitGroup
	activeWG     sync.WaitGroup
	stateFile    string
	shutdownChan chan os.Signal
	Events       chan string
}

func (j *Job) progressReporter() {
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-j.ctx.Done():
			return
		case <-ticker.C:
			elapsed := time.Since(j.stats.StartTime).Seconds()
			speed := 0.0
			if elapsed > 0 {
				speed = float64(j.stats.DownloadedBytes) / elapsed
			}

			msg := fmt.Sprintf("–§–∞–π–ª–æ–≤: %d | –°–∫–æ—Ä–æ—Å—Ç—å: %.2f KB/s | –í –æ—á–µ—Ä–µ–¥–∏: %d",
				j.stats.TotalFiles, speed/1024, len(j.pending))

			// –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∫–∞–Ω–∞–ª–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å CLI)
			if j.Events != nil {
				select {
				case j.Events <- msg:
				default:
				}
			} else {
				log.Println(msg) // Fallback –¥–ª—è CLI
			}
		}
	}
}
func NewJob(root string, cfg Config) (*Job, error) {
	parsed, err := url.Parse(root)
	if err != nil {
		return nil, err
	}

	id := ContentHash([]byte(root))[:8]
	stateFile := filepath.Join(cfg.OutputDir, id+StateFileExtension)

	filter := &DefaultURLFilter{
		domain:   parsed.Host,
		basePath: parsed.Path,
	}

	ctx, cancel := context.WithCancel(context.Background())

	job := &Job{
		ID:           id,
		RootURL:      root,
		Config:       cfg,
		Filter:       filter,
		Parsers:      []ContentParser{&HTMLParser{}, &CSSParser{}},
		Handlers:     []ContentHandler{&LinkRewriterHandlerV2{outputDir: cfg.OutputDir, analyzer: NewStrategyAnalyzer()}},
		Downloader:   NewDownloader(cfg),
		BasePath:     parsed.Path,
		pending:      make(chan string, 5000),
		visited:      make(map[string]bool),
		hashes:       make(map[string]bool),
		depths:       make(map[string]int),
		stats:        JobStats{FileTypes: make(map[string]int64), StartTime: time.Now()},
		ctx:          ctx,
		cancel:       cancel,
		stateFile:    stateFile,
		shutdownChan: make(chan os.Signal, 1),
		Events:       make(chan string, 100),
	}

	// –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
	if err := job.loadState(); err == nil {
		log.Printf("‚úÖ Resumed job %s from state file", id)
	} else {
		// –ù–∞—á–∏–Ω–∞–µ–º —Å –∫–æ—Ä–Ω–µ–≤–æ–≥–æ URL
		normalized, _ := NormalizeURL(root)
		job.activeWG.Add(1) // –î–æ–±–∞–≤–ª—è–µ–º –≤ WaitGroup –¥–ª—è rootURL
		job.pending <- normalized
		job.depths[normalized] = 0
		job.visited[normalized] = true
		log.Printf("üöÄ New job started for %s", root)
	}

	return job, nil
}

func (j *Job) Run() {
	signal.Notify(j.shutdownChan, os.Interrupt, syscall.SIGTERM)

	// –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
	go func() {
		<-j.shutdownChan
		log.Println("‚ö†Ô∏è  Shutdown signal received, saving state...")
		j.cancel()
	}()

	// –ü–ï–†–í–´–ú –∑–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–ø–æ—Ä—Ç–µ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–¥–ª—è GUI)
	go j.progressReporter()

	// –ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–æ–≤
	for i := 0; i < j.Config.Workers; i++ {
		j.wg.Add(1)
		go j.worker()
	}

	// –ì–æ—Ä—É—Ç–∏–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –∫–∞–Ω–∞–ª–∞ –ø—Ä–∏ –æ–ø—É—Å—Ç–æ—à–µ–Ω–∏–∏ –æ—á–µ—Ä–µ–¥–∏
	go func() {
		j.activeWG.Wait()
		log.Println("üì≠ All tasks completed, closing pending channel")
		close(j.pending)
	}()

	// –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
	j.wg.Wait()

	// –û—Ç–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á—Ç–æ–±—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å progressReporter
	j.cancel()

	// –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ GUI
	if j.Events != nil {
		j.Events <- "‚úÖ Download completed successfully!"
	}

	// –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
	if err := j.saveState(); err != nil {
		log.Printf("Error saving state: %v", err)
	}

	log.Println("‚úÖ Download completed. All links rewritten for local viewing.")
}

func (j *Job) worker() {
	defer j.wg.Done()

	for urlStr := range j.pending {
		j.processURL(urlStr)
		j.activeWG.Done()
	}
}

func (j *Job) processURL(urlStr string) {
	depth := j.depths[urlStr]
	log.Printf("Processing: %s (depth %d)", urlStr, depth)

	if depth > j.Config.MaxDepth {
		atomic.AddInt64(&j.stats.Skipped, 1)
		log.Printf("Max depth reached for %s", urlStr)
		return
	}

	// –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª - –ë–ï–ó –∏–∑–º–µ–Ω–µ–Ω–∏–π URL!
	content, contentType, err := j.Downloader.Download(j.ctx, urlStr)
	if err != nil {
		log.Printf("Download failed for %s: %v", urlStr, err)
		atomic.AddInt64(&j.stats.Failed, 1)
		return
	}

	// –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É
	hash := ContentHash(content)
	j.mu.Lock()
	if j.hashes[hash] {
		j.mu.Unlock()
		atomic.AddInt64(&j.stats.Skipped, 1)
		log.Printf("Duplicate content for %s", urlStr)
		return
	}
	j.hashes[hash] = true
	j.mu.Unlock()

	// –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
	meta := FileMetadata{
		URL:         urlStr,
		ContentType: contentType,
		Hash:        hash,
		Depth:       depth,
	}

	// –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
	modifiedContent := content
	for _, handler := range j.sortedHandlers() {
		modified, err := handler.Handle(modifiedContent, meta)
		if err != nil {
			log.Printf("Handler error for %s: %v", urlStr, err)
		} else {
			modifiedContent = modified
		}
	}

	savedPath, err := SaveFileV2(j.Config.OutputDir, urlStr, modifiedContent, contentType)
	if err != nil {
		log.Printf("Save failed for %s: %v", urlStr, err)
		atomic.AddInt64(&j.stats.Failed, 1)
		return
	}

	// –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
	atomic.AddInt64(&j.stats.TotalFiles, 1)
	atomic.AddInt64(&j.stats.DownloadedBytes, int64(len(content)))

	j.mu.Lock()
	j.stats.FileTypes[contentType]++
	j.mu.Unlock()

	log.Printf("‚úÖ Saved: %s ‚Üí %s", urlStr, savedPath)

	// –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç!)
	if depth < j.Config.MaxDepth {
		j.parseAndQueueLinks(content, contentType, urlStr, depth)
	}
}

func (j *Job) parseAndQueueLinks(content []byte, contentType, baseURL string, depth int) {
	for _, parser := range j.Parsers {
		if parser.CanParse(contentType) {
			rawLinks, err := parser.Parse(content, baseURL)
			if err != nil {
				log.Printf("Parse error for %s: %v", baseURL, err)
				continue
			}

			log.Printf("Found %d raw links in %s", len(rawLinks), baseURL)

			for _, rawLink := range rawLinks {
				// –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
				normalized, err := NormalizeURL(rawLink)
				if err != nil {
					continue
				}

				// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
				if !j.Filter.ShouldDownload(normalized) {
					reason := j.Filter.FilterReason(normalized)
					log.Printf("Filtered out: %s (%s)", normalized, reason)
					atomic.AddInt64(&j.stats.Skipped, 1)
					continue
				}

				// –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
				j.mu.Lock()
				if !j.visited[normalized] {
					j.visited[normalized] = true
					j.depths[normalized] = depth + 1

					select {
					case j.pending <- normalized:
						j.activeWG.Add(1) // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–∏–ª–∏ –≤ –æ—á–µ—Ä–µ–¥—å
						log.Printf("Enqueued: %s (depth %d)", normalized, depth+1)
					default:
						// –ï—Å–ª–∏ –∫–∞–Ω–∞–ª –ø–æ–ª–æ–Ω –∏–ª–∏ —É–∂–µ –∑–∞–∫—Ä—ã—Ç, –º—ã –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ WaitGroup.
						// –ù–µ—Ç –Ω—É–∂–¥—ã –≤ j.activeWG.Done(), —Ç–∞–∫ –∫–∞–∫ j.activeWG.Add(1) –Ω–µ –≤—ã–∑—ã–≤–∞–ª—Å—è.
						log.Printf("Queue full or closed, dropping: %s", normalized)
					}
				}
				j.mu.Unlock()
			}

			break // –û–±—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–∞—Ä—Å–µ—Ä
		}
	}
}

func (j *Job) sortedHandlers() []ContentHandler {
	handlers := make([]ContentHandler, len(j.Handlers))
	copy(handlers, j.Handlers)
	sort.Slice(handlers, func(i, k int) bool {
		return handlers[i].Priority() < handlers[k].Priority()
	})
	return handlers
}

func (j *Job) saveState() error {
	j.mu.Lock()
	defer j.mu.Unlock()

	// –°–ª–∏–≤–∞–µ–º –æ—á–µ—Ä–µ–¥—å –≤ —Å—Ä–µ–∑
	var pendingURLs []string
	for {
		select {
		case url := <-j.pending:
			pendingURLs = append(pendingURLs, url)
		default:
			// –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–∞–Ω–∞–ª –ø–æ—Å–ª–µ —Å–ª–∏–≤–∞
			j.pending = make(chan string, 5000)
			for _, url := range pendingURLs {
				j.pending <- url
			}

			// –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
			state := JobState{
				ID:          j.ID,
				RootURL:     j.RootURL,
				PendingURLs: pendingURLs,
				DepthMap:    j.depths,
				Stats:       j.stats,
				Config:      j.Config,
			}

			data, err := json.MarshalIndent(state, "", "  ")
			if err != nil {
				return err
			}

			return ioutil.WriteFile(j.stateFile, data, 0644)
		}
	}
}

func (j *Job) loadState() error {
	data, err := ioutil.ReadFile(j.stateFile)
	if err != nil {
		return err
	}

	var state JobState
	if err := json.Unmarshal(data, &state); err != nil {
		return err
	}

	j.ID = state.ID
	j.RootURL = state.RootURL
	j.stats = state.Stats
	j.Config = state.Config

	j.mu.Lock()
	defer j.mu.Unlock()

	// –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –∏ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL
	j.depths = make(map[string]int)
	j.visited = make(map[string]bool)
	j.hashes = make(map[string]bool)

	for url, depth := range state.DepthMap {
		j.depths[url] = depth
		j.visited[url] = true
	}

	// –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—á–µ—Ä–µ–¥—å
	j.pending = make(chan string, 5000)
	for _, url := range state.PendingURLs {
		j.pending <- url
		j.activeWG.Add(1) // –î–æ–±–∞–≤–ª—è–µ–º –≤ activeWG –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ URL
	}

	// –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –∏ –ø–∞—Ä—Å–µ—Ä—ã
	parsed, _ := url.Parse(j.RootURL)
	j.Filter = &DefaultURLFilter{
		domain:   parsed.Host,
		basePath: parsed.Path,
	}
	j.BasePath = parsed.Path

	// –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º LinkRewriterHandlerV2 –≤–º–µ—Å—Ç–æ LinkRewriterHandler
	j.Handlers = []ContentHandler{&LinkRewriterHandlerV2{
		outputDir: j.Config.OutputDir,
		analyzer:  NewStrategyAnalyzer(),
	}}
	j.Parsers = []ContentParser{&HTMLParser{}, &CSSParser{}}

	return nil
}

func drainChannel(ch chan string) []string {
	var urls []string
	for {
		select {
		case url := <-ch:
			urls = append(urls, url)
		default:
			return urls
		}
	}
}

// CLI –∫–æ–º–∞–Ω–¥—ã
var rootCmd = &cobra.Command{
	Use:   "downloader",
	Short: "Website Downloader with .php to .html conversion",
}

var downloadCmd = &cobra.Command{
	Use:   "download <url>",
	Short: "Download a website",
	Args:  cobra.ExactArgs(1),
	Run: func(cmd *cobra.Command, args []string) {
		cfg := loadConfig()

		// –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
		if err := os.MkdirAll(cfg.OutputDir, 0755); err != nil {
			log.Fatalf("Failed to create output directory: %v", err)
		}

		job, err := NewJob(args[0], cfg)
		if err != nil {
			log.Fatalf("Failed to create job: %v", err)
		}

		job.Run()
	},
}

var resumeCmd = &cobra.Command{
	Use:   "resume <job-id>",
	Short: "Resume a previous download job",
	Args:  cobra.ExactArgs(1),
	Run: func(cmd *cobra.Command, args []string) {
		cfg := loadConfig()

		job := &Job{
			ID:        args[0],
			Config:    cfg,
			stateFile: filepath.Join(cfg.OutputDir, args[0]+StateFileExtension),
		}

		if err := job.loadState(); err != nil {
			log.Fatalf("Failed to load job state: %v", err)
		}

		// –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∫–∞–Ω–∞–ª—ã
		job.ctx, job.cancel = context.WithCancel(context.Background())
		job.shutdownChan = make(chan os.Signal, 1)

		// –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
		job.Downloader = NewDownloader(cfg)

		// –î–û–ë–ê–í–¨–¢–ï: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
		job.Handlers = []ContentHandler{&LinkRewriterHandlerV2{
			outputDir: cfg.OutputDir,
			analyzer:  NewStrategyAnalyzer(),
		}}

		log.Printf("Resuming job %s for %s", job.ID, job.RootURL)
		job.Run()
	},
}

func loadConfig() Config {
	// –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
	viper.SetDefault("workers", DefaultWorkers)
	viper.SetDefault("max_depth", DefaultMaxDepth)
	viper.SetDefault("retries", DefaultRetries)
	viper.SetDefault("delay", DefaultDelay)
	viper.SetDefault("max_file_size", DefaultMaxFileSize)
	viper.SetDefault("output_dir", "./downloads")
	viper.SetDefault("user_agent", DefaultUserAgent)

	// –ß—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
	viper.SetConfigName("config")
	viper.SetConfigType("yaml")
	viper.AddConfigPath(".")
	viper.ReadInConfig() // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç

	return Config{
		Workers:     viper.GetInt("workers"),
		MaxDepth:    viper.GetInt("max_depth"),
		Retries:     viper.GetInt("retries"),
		Delay:       viper.GetDuration("delay"),
		MaxFileSize: viper.GetInt64("max_file_size"),
		OutputDir:   viper.GetString("output_dir"),
		UserAgent:   viper.GetString("user_agent"),
	}
}

func init() {
	// –§–ª–∞–≥–∏ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã download
	downloadCmd.Flags().Int("workers", DefaultWorkers, "Number of concurrent workers")
	downloadCmd.Flags().Int("max-depth", DefaultMaxDepth, "Maximum recursion depth")
	downloadCmd.Flags().Int("retries", DefaultRetries, "Retry attempts per URL")
	downloadCmd.Flags().Duration("delay", DefaultDelay, "Delay between requests")
	downloadCmd.Flags().Int64("max-file-size", DefaultMaxFileSize, "Maximum file size in bytes")
	downloadCmd.Flags().String("output-dir", "./downloads", "Output directory")
	downloadCmd.Flags().String("user-agent", DefaultUserAgent, "HTTP User-Agent header")

	// –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ–ª–∞–≥–æ–≤ –∫ viper
	viper.BindPFlags(downloadCmd.Flags())

	// –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
	rootCmd.AddCommand(downloadCmd, resumeCmd)
}

func main() {
	if err := rootCmd.Execute(); err != nil {
		log.Fatal(err)
	}
}
