package downloader

import (
	"bytes"
	"context"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"math/rand"
	"net/http"
	"path"
    "path/filepath"
	"net/url"
	"os"
	"os/signal"
	"regexp"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"golang.org/x/net/html"
)


const (
	DefaultWorkers     = 5 // –°–Ω–∏–∂–∞–µ–º —Å 10 –¥–æ 5 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
	DefaultMaxDepth    = 10
	DefaultRetries     = 3
	DefaultDelay       = 500 * time.Millisecond
	DefaultMaxFileSize = 10 * 1024 * 1024 // 10MB
	DefaultUserAgent   = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
	StateFileExtension = ".state.json"
)

var (
	ErrInvalidURL     = errors.New("invalid URL")
	ErrDownloadFailed = errors.New("download failed after retries")
	ErrParseFailed    = errors.New("parsing failed")
)

type FileMetadata struct {
	URL         string
	ContentType string
	Hash        string
	Depth       int
}

type JobStats struct {
	TotalFiles      int64
	DownloadedBytes int64
	Failed          int64
	Skipped         int64
	Speed           float64
	ETA             time.Duration
	FileTypes       map[string]int64
	StartTime       time.Time
}

type JobState struct {
	ID          string
	RootURL     string
	PendingURLs []string
	DepthMap    map[string]int
	Stats       JobStats
	Config      Config
}

type Config struct {
	Workers     int
	MaxDepth    int
	Retries     int
	Delay       time.Duration
	MaxFileSize int64
	OutputDir   string
	UserAgent   string
}

type ContentParser interface {
	CanParse(contentType string) bool
	Parse(content []byte, baseURL string) ([]string, error)
}

type URLFilter interface {
	ShouldDownload(url string) bool
	FilterReason(url string) string
}

type ContentHandler interface {
	Priority() int
	Handle(content []byte, meta FileMetadata) ([]byte, error)
}

// HTMLParser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –°–´–†–´–• —Å—Å—ã–ª–æ–∫ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
type HTMLParser struct{}

func (p *HTMLParser) CanParse(ct string) bool { return strings.Contains(ct, "text/html") }

func (p *HTMLParser) Parse(content []byte, baseURL string) ([]string, error) {
	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return nil, ErrParseFailed
	}
	var links []string
	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			switch n.Data {
			case "a", "link":
				for _, a := range n.Attr {
					if a.Key == "href" {
						links = append(links, a.Val)
					}
				}
			case "img", "script", "source":
				for _, a := range n.Attr {
					if a.Key == "src" {
						links = append(links, a.Val)
					}
				}
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)

	// –í–æ–∑–≤—Ä–∞—â–∞–µ–º –°–´–†–´–ï —Å—Å—ã–ª–∫–∏ (–±–µ–∑ –∑–∞–º–µ–Ω—ã .php ‚Üí .html)
	return resolveRawLinks(links, baseURL), nil
}

type CSSParser struct{}

func (p *CSSParser) CanParse(ct string) bool { return strings.Contains(ct, "text/css") }

func (p *CSSParser) Parse(content []byte, baseURL string) ([]string, error) {
	re := regexp.MustCompile(`(?i)url\s*\(\s*['"]?([^'")]+)['"]?\s*\)`)
	matches := re.FindAllSubmatch(content, -1)
	var links []string
	for _, m := range matches {
		if len(m[1]) > 0 {
			links = append(links, string(m[1]))
		}
	}
	return resolveRawLinks(links, baseURL), nil
}

// resolveRawLinks ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –ë–ï–ó –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
func resolveRawLinks(links []string, baseURL string) []string {
	var resolved []string
	base, _ := url.Parse(baseURL)
	bad := []string{"devnull", "410011174743222", "yoomoney", "t.me/metanitcom"}

	for _, l := range links {
		l = strings.TrimSpace(l)
		if strings.HasPrefix(l, "data:") || strings.HasPrefix(l, "#") || strings.HasPrefix(l, "javascript:") {
			continue
		}
		// Handle protocol-relative URLs
		if strings.HasPrefix(l, "//") {
			l = "https:" + l
		}
		u, err := url.Parse(l)
		if err != nil {
			continue
		}
		res := base.ResolveReference(u).String()

		skip := false
		for _, p := range bad {
			if strings.Contains(res, p) {
				skip = true
				break
			}
		}
		if !skip {
			resolved = append(resolved, res)
			log.Printf("Resolved RAW link: %s", res)
		}
	}
	return resolved
}

func replacePhpToHtmlLinks(content []byte, baseURL string) ([]byte, error) {
	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return content, nil
	}

	baseParsed, _ := url.Parse(baseURL)

	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			for i := range n.Attr {
				attr := &n.Attr[i]
				if attr.Key == "href" || attr.Key == "src" || attr.Key == "action" {
					orig := attr.Val

					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
					if strings.HasPrefix(orig, "data:") ||
						strings.HasPrefix(orig, "#") ||
						strings.HasPrefix(orig, "javascript:") ||
						strings.HasPrefix(orig, "mailto:") ||
						strings.HasPrefix(orig, "tel:") {
						continue
					}

					// –†–∞–∑–±–∏—Ä–∞–µ–º URL
					u, err := url.Parse(orig)
					if err != nil {
						continue
					}

					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
					if u.Host != "" && u.Host != baseParsed.Host {
						continue
					}

					// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—É—Ç—å
					path := u.Path

					// –ï—Å–ª–∏ –ø—É—Ç—å –ø—É—Å—Ç–æ–π –∏–ª–∏ –∫–æ—Ä–Ω–µ–≤–æ–π
					if path == "" || path == "/" {
						// –î–ª—è –∫–æ—Ä–Ω—è –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
						u.Path = "/"
						attr.Val = u.String()
						continue
					}

					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ—Å—É—Ä—Å—ã (CSS, JS, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
					// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
					lowerPath := strings.ToLower(path)
					isResource := strings.HasSuffix(lowerPath, ".css") ||
						strings.HasSuffix(lowerPath, ".js") ||
						strings.HasSuffix(lowerPath, ".mjs") ||
						strings.HasSuffix(lowerPath, ".cjs") ||
						strings.HasSuffix(lowerPath, ".png") ||
						strings.HasSuffix(lowerPath, ".jpg") ||
						strings.HasSuffix(lowerPath, ".jpeg") ||
						strings.HasSuffix(lowerPath, ".gif") ||
						strings.HasSuffix(lowerPath, ".svg") ||
						strings.HasSuffix(lowerPath, ".ico") ||
						strings.HasSuffix(lowerPath, ".woff") ||
						strings.HasSuffix(lowerPath, ".woff2") ||
						strings.HasSuffix(lowerPath, ".ttf") ||
						strings.HasSuffix(lowerPath, ".eot") ||
						strings.HasSuffix(lowerPath, ".otf") ||
						strings.HasSuffix(lowerPath, ".mp4") ||
						strings.HasSuffix(lowerPath, ".webm") ||
						strings.HasSuffix(lowerPath, ".mp3") ||
						strings.HasSuffix(lowerPath, ".wav") ||
						strings.HasSuffix(lowerPath, ".ogg")

					if isResource {
						// –î–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –∫–∞–∫ –µ—Å—Ç—å
						continue
					}

					// –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º .php —Å—Å—ã–ª–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü)
					if strings.HasSuffix(lowerPath, ".php") {
						// –£–±–∏—Ä–∞–µ–º .php
						newPath := strings.TrimSuffix(path, ".php")

						// –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª "index.php", –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ "/"
						if strings.HasSuffix(strings.ToLower(newPath), "/index") {
							newPath = strings.TrimSuffix(newPath, "/index")
							if newPath == "" {
								newPath = "/"
							}
						} else if strings.EqualFold(newPath, "index") {
							// –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ "index.php", —Ç–æ–∂–µ –≤ "/"
							newPath = "/"
						}

						u.Path = newPath
						attr.Val = u.String()
						log.Printf("üîó Rewrote PHP link: %s ‚Üí %s", orig, attr.Val)
					} else if strings.HasSuffix(lowerPath, ".html") ||
						strings.HasSuffix(lowerPath, ".htm") {
						// –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º .html —Å—Å—ã–ª–∫–∏
						// –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
						newPath := strings.TrimSuffix(
							strings.TrimSuffix(path, ".html"), ".htm")

						// –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª "index.html", –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ "/"
						if strings.HasSuffix(strings.ToLower(newPath), "/index") {
							newPath = strings.TrimSuffix(newPath, "/index")
							if newPath == "" {
								newPath = "/"
							}
						} else if strings.EqualFold(newPath, "index") {
							// –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ "index.html", —Ç–æ–∂–µ –≤ "/"
							newPath = "/"
						}

						u.Path = newPath
						attr.Val = u.String()
						log.Printf("üîó Rewrote HTML link: %s ‚Üí %s", orig, attr.Val)
					}
				}
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)

	var buf bytes.Buffer
	html.Render(&buf, doc)
	return buf.Bytes(), nil
}

// FileSaveStrategy - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
type FileSaveStrategy interface {
	ShouldSaveAsDirectory(url string, contentType string) bool
	GetSavePath(outputDir string, url string, contentType string) (string, string) // –ø—É—Ç—å, –∏–º—è —Ñ–∞–π–ª–∞
	RewriteLink(originalURL, baseURL string) string
}

// DirectoryIndexStrategy - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è "–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å index.html"
type DirectoryIndexStrategy struct{}

func (s *DirectoryIndexStrategy) ShouldSaveAsDirectory(urlStr string, contentType string) bool {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		return false
	}
	path := parsed.Path

	// –î–ª—è HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
	if strings.Contains(contentType, "text/html") {
		return true
	}

	// –î–ª—è .php —Ñ–∞–π–ª–æ–≤ —Ç–æ–∂–µ (–¥–∞–∂–µ –µ—Å–ª–∏ content-type –Ω–µ —É–∫–∞–∑–∞–Ω)
	if strings.HasSuffix(strings.ToLower(path), ".php") {
		return true
	}

	// –î–ª—è –ø—É—Ç–µ–π –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ä–µ—Å—É—Ä—Å–∞–º–∏
	if !strings.Contains(path, ".") && path != "/" && path != "" {
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ API endpoint –∏–ª–∏ –ø–æ–¥–æ–±–Ω—ã–º
		if !strings.Contains(path, "/api/") &&
			!strings.Contains(path, "/ajax/") &&
			!strings.Contains(path, "/rest/") {
			return true
		}
	}

	return false
}

func (s *DirectoryIndexStrategy) GetSavePath(outputDir string, urlStr string, contentType string) (string, string) {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		log.Printf("Parse error in GetSavePath: %v", err)
		return "", ""
	}
	host := parsed.Host
	path := parsed.Path

	// –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
	if path == "" || path == "/" {
		path = "/"
	}

	cleanPath := strings.TrimPrefix(path, "/")
	if cleanPath == "" {
		cleanPath = "index"
	}

	// –†–∞–∑–¥–µ–ª—è–µ–º –ø—É—Ç—å –Ω–∞ —á–∞—Å—Ç–∏
	var parts []string
	if cleanPath != "" {
		parts = strings.Split(cleanPath, "/")
	}

	// –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
	if len(parts) > 0 {
		lastPart := parts[len(parts)-1]

		// –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü
		lowerLast := strings.ToLower(lastPart)
		if strings.HasSuffix(lowerLast, ".php") ||
			strings.HasSuffix(lowerLast, ".html") ||
			strings.HasSuffix(lowerLast, ".htm") ||
			strings.HasSuffix(lowerLast, ".asp") ||
			strings.HasSuffix(lowerLast, ".aspx") ||
			strings.HasSuffix(lowerLast, ".jsp") {

			// –£–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
			ext := filepath.Ext(lastPart)
			newName := strings.TrimSuffix(lastPart, ext)

			if newName == "" || strings.EqualFold(newName, "index") {
				parts = parts[:len(parts)-1]
			} else {
				parts[len(parts)-1] = newName
			}
		} else if strings.EqualFold(lastPart, "index") {
			parts = parts[:len(parts)-1]
		}
	}

	// –°—Ç—Ä–æ–∏–º –ø—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
	basePath := filepath.Join(outputDir, host)

	var saveDir string
	if len(parts) > 0 {
		saveDir = filepath.Join(append([]string{basePath}, parts...)...)
	} else {
		saveDir = basePath
	}

	return saveDir, "index.html"
}

func (s *DirectoryIndexStrategy) RewriteLink(originalURL, baseURL string) string {
    parsed, err1 := url.Parse(originalURL)
    baseParsed, err2 := url.Parse(baseURL)

    if err1 != nil || err2 != nil {
        return originalURL
    }

    // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
    if parsed.Host != "" && parsed.Host != baseParsed.Host {
        return originalURL
    }

    if strings.HasPrefix(originalURL, "#") ||
        strings.HasPrefix(originalURL, "javascript:") ||
        strings.HasPrefix(originalURL, "mailto:") ||
        strings.HasPrefix(originalURL, "tel:") ||
        strings.HasPrefix(originalURL, "data:") {
        return originalURL
    }

    // –í–ê–ñ–ù–û: calculateRelativePath —Ç–µ–ø–µ—Ä—å –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –ü–û–õ–ù–´–• URL
    // –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ –ø–æ–Ω–∏–º–∞—Ç—å, –≥–¥–µ —Ñ–∞–π–ª—ã –ª–µ–∂–∞—Ç –Ω–∞ –¥–∏—Å–∫–µ.

    // –§–æ—Ä–º–∏—Ä—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL –¥–ª—è —Ü–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
    targetURL := originalURL
    if !strings.HasPrefix(originalURL, "http") {
        resolved := baseParsed.ResolveReference(parsed)
        targetURL = resolved.String()
    }

    // 1. –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
    relPath, err := calculateRelativePath(baseURL, targetURL)
    if err != nil {
        return originalURL
    }

    // 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è DirectoryIndex (—É–±–∏—Ä–∞–µ–º .html/index.html –∏–∑ —Å—Å—ã–ª–æ–∫)
    // –ß—Ç–æ–±—ã —Å—Å—ã–ª–∫–∏ –≤ –±—Ä–∞—É–∑–µ—Ä–µ –≤—ã–≥–ª—è–¥–µ–ª–∏ –∫–∞–∫ href="../assets/" –≤–º–µ—Å—Ç–æ "../assets/index.html"

    // –£–±–∏—Ä–∞–µ–º "index.html" –∏–∑ –∫–æ–Ω—Ü–∞, –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
    if strings.HasSuffix(relPath, "index.html") {
        relPath = strings.TrimSuffix(relPath, "index.html")
    }

    // –ï—Å–ª–∏ –ø—É—Ç—å —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º (—Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç—É –∂–µ –ø–∞–ø–∫—É), —Å—Ç–∞–≤–∏–º "./"
    if relPath == "" {
        relPath = "./"
    }

    // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø—É—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è Query-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (?v=1.2), –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏
    parsed.Path = relPath
    parsed.Scheme = "" // –î–µ–ª–∞–µ–º —Å—Å—ã–ª–∫—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π
    parsed.Host = ""

    return parsed.String()
}

// –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è URL
func calculateRelativePath(sourceURL, targetURL string) (string, error) {
    s, err := url.Parse(sourceURL)
    t, err := url.Parse(targetURL)
    if err != nil {
        return targetURL, err
    }

    // –ï—Å–ª–∏ –¥–æ–º–µ–Ω—ã —Ä–∞–∑–Ω—ã–µ, –æ—Å—Ç–∞–≤–ª—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—É—é —Å—Å—ã–ª–∫—É
    if s.Host != t.Host {
        return targetURL, nil
    }

    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º "–ø—É—Ç–∏" –Ω–∞ –¥–∏—Å–∫–µ –¥–ª—è –æ–±–æ–∏—Ö —Ñ–∞–π–ª–æ–≤
    sourcePath := getDiskPath(s)
    targetPath := getDiskPath(t)

    // –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∏–∑ –ø–∞–ø–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∫ —Ñ–∞–π–ª—É —Ü–µ–ª–∏
    rel, err := filepath.Rel(filepath.Dir(sourcePath), targetPath)
    if err != nil {
        return targetURL, err
    }

    // –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (\ –≤ Windows) –≤ URL-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (/)
    return filepath.ToSlash(rel), nil
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –ª–æ–≥–∏–∫—É SaveFileV2
func getDiskPath(u *url.URL) string {
    p := u.Path
    if p == "" || p == "/" {
        return "index.html"
    }

    // –û—á–∏—â–∞–µ–º –ø—É—Ç—å –æ—Ç –¥–≤–æ–π–Ω—ã—Ö —Å–ª—ç—à–µ–π –∏ –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    p = path.Clean(p)
    if p == "." {
        return "index.html"
    }

    // –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å–ª—ç—à, —á—Ç–æ–±—ã filepath.Join –Ω–µ —Å—á–∏—Ç–∞–ª –ø—É—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º
    p = strings.TrimPrefix(p, "/")

    // –ï—Å–ª–∏ —ç—Ç–æ –ø–∞–ø–∫–∞ (URL –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ /) –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    // –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ—á–∫–∏ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–µ–≥–º–µ–Ω—Ç–µ –ø—É—Ç–∏
    lastSegment := path.Base(p)
    if strings.HasSuffix(u.Path, "/") || !strings.Contains(lastSegment, ".") {
        // –ï—Å–ª–∏ —ç—Ç–æ php, –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ html, –∏–Ω–∞—á–µ –¥–µ–ª–∞–µ–º index.html –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏
        if strings.HasSuffix(strings.ToLower(p), ".php") {
            return strings.TrimSuffix(p, ".php") + ".html"
        }
        return path.Join(p, "index.html")
    }

    return p
}

// FileOnlyStrategy - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è "–ø—Ä–æ—Å—Ç–æ —Ñ–∞–π–ª" –¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤
type FileOnlyStrategy struct{}

func (s *FileOnlyStrategy) ShouldSaveAsDirectory(urlStr string, contentType string) bool {
	// –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ñ–∞–π–ª
	return false
}

func (s *FileOnlyStrategy) GetSavePath(outputDir string, urlStr string, contentType string) (string, string) {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		log.Printf("Parse error in FileOnlyStrategy: %v", err)
		return "", ""
	}
	host := parsed.Host
	path := parsed.Path

	if path == "" || path == "/" {
		path = "/index.html"
	}

	cleanPath := strings.TrimPrefix(path, "/")
	if cleanPath == "" {
		cleanPath = "index.html"
	}

	saveDir := filepath.Join(outputDir, host, filepath.Dir(cleanPath))
	fileName := filepath.Base(cleanPath)

	return saveDir, fileName
}

func (s *FileOnlyStrategy) RewriteLink(originalURL, baseURL string) string {
	// –î–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
	return originalURL
}

// StrategyAnalyzer - –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
type StrategyAnalyzer struct {
	strategies []FileSaveStrategy
}

func NewStrategyAnalyzer() *StrategyAnalyzer {
	return &StrategyAnalyzer{
		strategies: []FileSaveStrategy{
			&DirectoryIndexStrategy{},
			&FileOnlyStrategy{},
		},
	}
}

func (a *StrategyAnalyzer) Analyze(urlStr string, contentType string, content []byte) FileSaveStrategy {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		// –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å URL, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ñ–∞–π–ª–æ–≤
		return &FileOnlyStrategy{}
	}

	path := parsed.Path

	// –ê–Ω–∞–ª–∏–∑ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
	lowerPath := strings.ToLower(path)

	// –†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ñ–∞–π–ª—ã)
	resourceExtensions := []string{
		".css", ".js", ".mjs", ".cjs",
		".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp",
		".woff", ".woff2", ".ttf", ".eot", ".otf",
		".mp4", ".webm", ".mp3", ".wav", ".ogg", ".avi", ".mov",
		".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
		".zip", ".rar", ".7z", ".tar", ".gz",
		".json", ".xml", ".txt", ".csv",
	}

	for _, ext := range resourceExtensions {
		if strings.HasSuffix(lowerPath, ext) {
			return &FileOnlyStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type
	if contentType != "" {
		// –†–µ—Å—É—Ä—Å–Ω—ã–µ Content-Type
		resourceContentTypes := []string{
			"text/css",
			"application/javascript", "application/x-javascript",
			"image/", "font/", "audio/", "video/",
			"application/pdf", "application/zip",
			"application/json", "application/xml",
		}

		for _, ct := range resourceContentTypes {
			if strings.Contains(contentType, ct) {
				return &FileOnlyStrategy{}
			}
		}

		// HTML Content-Type
		if strings.Contains(contentType, "text/html") {
			return &DirectoryIndexStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 3: –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–µ—Å–ª–∏ Content-Type –Ω–µ —É–∫–∞–∑–∞–Ω)
	if contentType == "" || contentType == "application/octet-stream" {
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –±–∞–π—Ç—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ HTML —Ç–µ–≥–æ–≤
		contentStr := string(content)
		if len(contentStr) > 100 {
			sample := strings.ToLower(contentStr[:100])
			if strings.Contains(sample, "<!doctype") ||
				strings.Contains(sample, "<html") ||
				strings.Contains(sample, "<head") ||
				strings.Contains(sample, "<body") {
				return &DirectoryIndexStrategy{}
			}
		}

		// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤ URL
		if strings.HasSuffix(lowerPath, ".php") ||
			strings.HasSuffix(lowerPath, ".html") ||
			strings.HasSuffix(lowerPath, ".htm") ||
			strings.HasSuffix(lowerPath, ".asp") ||
			strings.HasSuffix(lowerPath, ".aspx") ||
			strings.HasSuffix(lowerPath, ".jsp") {
			return &DirectoryIndexStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 4: –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—É—Ç–µ–π
	// –ï—Å–ª–∏ –ø—É—Ç—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–∏–ø–∏—á–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
	staticPatterns := []string{
		"/static/", "/assets/", "/public/", "/resources/",
		"/css/", "/js/", "/images/", "/img/", "/fonts/",
		"/uploads/", "/media/", "/downloads/",
	}

	for _, pattern := range staticPatterns {
		if strings.Contains(path, pattern) {
			return &FileOnlyStrategy{}
		}
	}

	// –ê–Ω–∞–ª–∏–∑ 5: –ü—É—Ç–∏ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
	if !strings.Contains(path, ".") && path != "/" && path != "" {
		// –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–∏–±–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è), –ª–∏–±–æ API endpoint
		// –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã API
		apiPatterns := []string{"/api/", "/ajax/", "/rest/", "/graphql", "/auth/"}
		for _, pattern := range apiPatterns {
			if strings.Contains(path, pattern) {
				return &FileOnlyStrategy{}
			}
		}

		// –ï—Å–ª–∏ –Ω–µ API, —Ç–æ —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
		return &DirectoryIndexStrategy{}
	}

	// –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
	return &DirectoryIndexStrategy{}
}

type DefaultURLFilter struct {
	domain   string
	basePath string
}

func (f *DefaultURLFilter) ShouldDownload(u string) bool {
    parsed, err := url.Parse(u)
    if err != nil {
        return false
    }

    // 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–∞ (–Ω–µ —Å–∫–∞—á–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å–∞–π—Ç—ã)
    if parsed.Host != f.domain {
        return false
    }

    pathLower := strings.ToLower(parsed.Path)

    // 2. –°–ø–∏—Å–æ–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (–∞—Å—Å–µ—Ç–æ–≤)
    assetExts := []string{
        ".css", ".js", ".mjs", ".json", ".map",
        ".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp", ".avif",
        ".woff", ".woff2", ".ttf", ".otf", ".eot",
        ".mp4", ".webm", ".mp3", ".wav", ".pdf",
    }

    // –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—Å—Å–µ—Ç ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑ –ª—é–±–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ —ç—Ç–æ–º –¥–æ–º–µ–Ω–µ
    for _, ext := range assetExts {
        if strings.HasSuffix(pathLower, ext) {
            return true
        }
    }

    // 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü (HTML, PHP –∏–ª–∏ URL –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
    // –†–∞–∑—Ä–µ—à–∞–µ–º, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –±–∞–∑–æ–≤–æ–π –ø–∞–ø–∫–∏ (basePath)
    isPage := strings.HasSuffix(pathLower, ".html") ||
              strings.HasSuffix(pathLower, ".php") ||
              !strings.Contains(filepath.Base(pathLower), ".")

    if isPage {
        return strings.HasPrefix(parsed.Path, f.basePath)
    }

    // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–∑—Ä–µ—à–∞–µ–º –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ, —á—Ç–æ –Ω–µ –ø–æ–ø–∞–ª–æ –≤ —Ñ–∏–ª—å—Ç—Ä —Å—Ç—Ä–∞–Ω–∏—Ü,
    // –Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –Ω–∞—à–µ–º –¥–æ–º–µ–Ω–µ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    return true
}

func (f *DefaultURLFilter) FilterReason(u string) string {
	return "outside base path or not asset"
}

type LinkRewriterHandlerV2 struct {
	outputDir string
	analyzer  *StrategyAnalyzer
}

func (h *LinkRewriterHandlerV2) Priority() int { return 10 }

func (h *LinkRewriterHandlerV2) Handle(content []byte, meta FileMetadata) ([]byte, error) {
	// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-HTML –∫–æ–Ω—Ç–µ–Ω—Ç
	if !strings.Contains(meta.ContentType, "text/html") {
		return content, nil
	}

	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return content, nil
	}

	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			for i := range n.Attr {
				attr := &n.Attr[i]
				if attr.Key == "href" || attr.Key == "src" || attr.Key == "action" {
					// –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏
					if attr.Val == "" {
						continue
					}

					// –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (file://) –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
					if strings.HasPrefix(attr.Val, "file://") {
						continue
					}

					// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –∏ –≤—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
					strategy := h.analyzer.Analyze(attr.Val, "", nil)
					// –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Å—Å—ã–ª–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
					newURL := strategy.RewriteLink(attr.Val, meta.URL)

					if newURL != attr.Val {
						// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
						if !strings.Contains(newURL, "://") && !strings.HasPrefix(newURL, "/") {
							// –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ
							if strings.HasPrefix(newURL, "./") || strings.HasPrefix(newURL, "../") {
								attr.Val = newURL
							} else {
								// –î–æ–±–∞–≤–ª—è–µ–º ./ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
								attr.Val = "./" + newURL
							}
						} else {
							attr.Val = newURL
						}
						log.Printf("üîó Rewrote link: %s ‚Üí %s (from: %s)", attr.Val, newURL, meta.URL)
					}
				}
			}
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)

	var buf bytes.Buffer
	html.Render(&buf, doc)
	return buf.Bytes(), nil
}

func SaveFileV2(outputDir string, urlStr string, data []byte, contentType string) (string, error) {
    parsed, err := url.Parse(urlStr)
    if err != nil || parsed.Host == "" {
        return "", fmt.Errorf("invalid URL or empty host")
    }

    // –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ –¥–æ–º–µ–Ω–∞
    relDiskPath := getDiskPath(parsed)

    // –°–æ–±–∏—Ä–∞–µ–º: output/wails.io/ru/index.html
    fullPath := filepath.Join(outputDir, parsed.Host, relDiskPath)

    if err := os.MkdirAll(filepath.Dir(fullPath), 0755); err != nil {
        return "", err
    }

    err = os.WriteFile(fullPath, data, 0644)
    if err != nil {
        return "", err
    }

    return relDiskPath, nil
}
func NormalizeURL(u string) (string, error) {
	pu, err := url.Parse(u)
	if err != nil {
		return "", err
	}

	pu.Fragment = ""

	path := pu.Path
	if path == "" {
		path = "/"
	}

	// Normalize index.html/index.htm paths
	lower := strings.ToLower(path)
	if strings.HasSuffix(lower, "/index.html") || strings.HasSuffix(lower, "/index.htm") {
		path = strings.TrimSuffix(path, "/index.html")
		path = strings.TrimSuffix(path, "/index.htm")
		if path == "" {
			path = "/"
		}
	} else if strings.HasSuffix(lower, "index.html") || strings.HasSuffix(lower, "index.htm") {
		path = strings.TrimSuffix(path, "index.html")
		path = strings.TrimSuffix(path, "index.htm")
		if path == "" {
			path = "/"
		}
	}

	pu.Path = path

	result := pu.String()
	log.Printf("üîó NormalizeURL: %s ‚Üí %s", u, result)
	return result, nil
}

func ContentHash(b []byte) string {
	h := sha256.Sum256(b)
	return hex.EncodeToString(h[:])
}

type Downloader struct {
	client    *http.Client
	retries   int
	delay     time.Duration
	maxSize   int64
	userAgent string
}

func NewDownloader(c Config) *Downloader {
	return &Downloader{
		client: &http.Client{
			Transport: &http.Transport{
				MaxIdleConns:    c.Workers * 2,
				IdleConnTimeout: 30 * time.Second,
			},
			CheckRedirect: func(r *http.Request, v []*http.Request) error {
				log.Printf("Redirect: %s ‚Üí %s", v[len(v)-1].URL, r.URL)
				return nil
			},
			Timeout: 30 * time.Second,
		},
		retries:   c.Retries,
		delay:     c.Delay,
		maxSize:   c.MaxFileSize,
		userAgent: c.UserAgent,
	}
}

func (d *Downloader) Download(ctx context.Context, u string) ([]byte, string, error) {
	log.Printf("DOWNLOAD REQUEST: %s", u)

	for attempt := 1; attempt <= d.retries; attempt++ {
		req, err := http.NewRequestWithContext(ctx, "GET", u, nil)
		if err != nil {
			log.Printf("Request creation error for %s: %v", u, err)
			return nil, "", err
		}

		req.Header.Set("User-Agent", d.userAgent)

		// –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–º–µ–Ω —Ü–µ–ª–µ–≤–æ–≥–æ URL –≤ –∫–∞—á–µ—Å—Ç–≤–µ Referer (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ)
		parsed, _ := url.Parse(u)
		req.Header.Set("Referer", parsed.Scheme+"://"+parsed.Host+"/")
		req.Header.Set("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
		req.Header.Set("Accept-Language", "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7")

		resp, err := d.client.Do(req)
		if err != nil {
			log.Printf("HTTP error for %s (attempt %d): %v", u, attempt, err)
			if attempt == d.retries {
				return nil, "", ErrDownloadFailed
			}
			time.Sleep(d.delay + time.Duration(rand.Intn(1000))*time.Millisecond)
			continue
		}

		log.Printf("RESPONSE: %s ‚Üí %d %s", u, resp.StatusCode, resp.Header.Get("Content-Type"))

		if resp.StatusCode != 200 {
			resp.Body.Close()
			if resp.StatusCode == 404 {
				log.Printf("‚ùå 404 Not Found: %s", u)
				return nil, "", fmt.Errorf("404 Not Found: %s", u)
			}
			log.Printf("HTTP error status %d for %s (attempt %d)", resp.StatusCode, u, attempt)

			if attempt == d.retries {
				return nil, "", fmt.Errorf("status %d", resp.StatusCode)
			}
			time.Sleep(d.delay + time.Duration(rand.Intn(1000))*time.Millisecond)
			continue
		}

		content, err := io.ReadAll(io.LimitReader(resp.Body, d.maxSize+1))
		resp.Body.Close()

		if err != nil {
			log.Printf("Read error for %s: %v", u, err)
			return nil, "", err
		}

		if len(content) > int(d.maxSize) {
			log.Printf("File too large: %s (%d bytes)", u, len(content))
			return nil, "", errors.New("file too large")
		}

		log.Printf("SUCCESS: Downloaded %s (%d bytes)", u, len(content))
		return content, resp.Header.Get("Content-Type"), nil
	}

	return nil, "", ErrDownloadFailed
}

type Job struct {
	ID         string
	RootURL    string
	Config     Config
	Filter     URLFilter
	Parsers    []ContentParser
	Handlers   []ContentHandler
	Downloader *Downloader
	BasePath   string

	mu           sync.Mutex
	pending      chan string
	visited      map[string]bool
	hashes       map[string]bool
	depths       map[string]int
	stats        JobStats
	ctx          context.Context
	cancel       context.CancelFunc
	wg           sync.WaitGroup
	activeWG     sync.WaitGroup
	stateFile    string
	shutdownChan chan os.Signal
	Events       chan string
}

func (j *Job) progressReporter() {
	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-j.ctx.Done():
			return
		case <-ticker.C:
			elapsed := time.Since(j.stats.StartTime).Seconds()
			speed := 0.0
			if elapsed > 0 {
				speed = float64(j.stats.DownloadedBytes) / elapsed
			}

			msg := fmt.Sprintf("–§–∞–π–ª–æ–≤: %d | –°–∫–æ—Ä–æ—Å—Ç—å: %.2f KB/s | –í –æ—á–µ—Ä–µ–¥–∏: %d",
				j.stats.TotalFiles, speed/1024, len(j.pending))

			j.sendLog(msg, false)
		}
	}
}

func (j *Job) sendLog(msg string, terminalOnly bool) {
	if !terminalOnly && j.Events != nil {
		select {
		case j.Events <- msg:
		default:
		}
	}
	log.Println(msg)
}
func NewJob(root string, cfg Config) (*Job, error) {
	parsed, err := url.Parse(root)
	if err != nil {
		return nil, err
	}

	id := ContentHash([]byte(root))[:8]
	stateFile := filepath.Join(cfg.OutputDir, id+StateFileExtension)

	filter := &DefaultURLFilter{
		domain:   parsed.Host,
		basePath: parsed.Path,
	}

	ctx, cancel := context.WithCancel(context.Background())

	job := &Job{
		ID:           id,
		RootURL:      root,
		Config:       cfg,
		Filter:       filter,
		Parsers:      []ContentParser{&HTMLParser{}, &CSSParser{}},
		Handlers:     []ContentHandler{&LinkRewriterHandlerV2{outputDir: cfg.OutputDir, analyzer: NewStrategyAnalyzer()}},
		Downloader:   NewDownloader(cfg),
		BasePath:     parsed.Path,
		pending:      make(chan string, 5000),
		visited:      make(map[string]bool),
		hashes:       make(map[string]bool),
		depths:       make(map[string]int),
		stats:        JobStats{FileTypes: make(map[string]int64), StartTime: time.Now()},
		ctx:          ctx,
		cancel:       cancel,
		stateFile:    stateFile,
		shutdownChan: make(chan os.Signal, 1),
		Events:       make(chan string, 100),
	}

	// –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
	if err := job.loadState(); err == nil {
		log.Printf("‚úÖ Resumed job %s from state file", id)
	} else {
		// –ù–∞—á–∏–Ω–∞–µ–º —Å –∫–æ—Ä–Ω–µ–≤–æ–≥–æ URL
		normalized, _ := NormalizeURL(root)
		job.activeWG.Add(1) // –î–æ–±–∞–≤–ª—è–µ–º –≤ WaitGroup –¥–ª—è rootURL
		job.pending <- normalized
		job.depths[normalized] = 0
		job.visited[normalized] = true
		log.Printf("üöÄ New job started for %s", root)
	}

	return job, nil
}

func (j *Job) Run() {
	if j.Events != nil {
		defer close(j.Events)
	}
	signal.Notify(j.shutdownChan, os.Interrupt, syscall.SIGTERM)

	// –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
	defer func() {
		j.wg.Wait()
		j.sendLog("üì≠ All tasks completed, closing pending channel", false)
		j.cancel()

		if j.Events != nil {
			j.Events <- "‚úÖ Download completed successfully!"
		}

		if err := j.saveState(); err != nil {
			log.Printf("Error saving state: %v", err)
		}
		log.Println("‚úÖ Download completed. All links rewritten for local viewing.")
	}()

	// –ü–ï–†–í–´–ú –∑–∞–ø—É—Å–∫–∞–µ–º —Ä–µ–ø–æ—Ä—Ç–µ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–¥–ª—è GUI)
	go j.progressReporter()

	// –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ—á–µ—Ä–µ–¥—å
	normalized, _ := NormalizeURL(j.RootURL)
	j.mu.Lock()
	j.depths[normalized] = 0
	j.visited[normalized] = true
	j.mu.Unlock()

	// Discover common files (404, robots, etc.)
	j.discoverCommonFiles()

	j.activeWG.Add(1)
	j.pending <- normalized

	// –ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–æ–≤
	for i := 0; i < j.Config.Workers; i++ {
		j.wg.Add(1)
		go j.worker()
	}

	j.activeWG.Wait()
	close(j.pending)
	j.wg.Wait()
}

func (j *Job) discoverCommonFiles() {
	commonPaths := []string{
		"/404", "/404.html", "/robots.txt", "/sitemap.xml", "/favicon.ico",
		"/apple-touch-icon.png", "/manifest.json",
	}

	parsed, err := url.Parse(j.RootURL)
	if err != nil {
		return
	}
	baseURL := parsed.Scheme + "://" + parsed.Host

	for _, p := range commonPaths {
		targetURL := baseURL + p
		j.mu.Lock()
		if _, exists := j.depths[targetURL]; !exists {
			j.depths[targetURL] = 0 // Treat as root level
			j.mu.Unlock()
			j.activeWG.Add(1)
			select {
			case j.pending <- targetURL:
				j.sendLog(fmt.Sprintf("[Discovery] Queued common file: %s", p), false)
			default:
				j.activeWG.Done()
			}
		} else {
			j.mu.Unlock()
		}
	}
}

func (j *Job) worker() {
	defer j.wg.Done()

	for urlStr := range j.pending {
		j.processURL(urlStr)
		j.activeWG.Done()
	}
}

func (j *Job) processURL(urlStr string) {
    j.mu.Lock()
    depth := j.depths[urlStr]
    j.mu.Unlock()

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –≤–∞–ª–∏–¥–Ω—ã–π –ø–µ—Ä–µ–¥ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º
    if !strings.HasPrefix(urlStr, "http") {
        j.sendLog(fmt.Sprintf("[Error] Invalid URL format: %s", urlStr), false)
        return
    }

    j.sendLog(fmt.Sprintf("[Info] Processing: %s (depth %d)", urlStr, depth), false)

    if depth > j.Config.MaxDepth {
        atomic.AddInt64(&j.stats.Skipped, 1)
        return
    }

    content, contentType, err := j.Downloader.Download(j.ctx, urlStr)
    if err != nil {
        j.sendLog(fmt.Sprintf("[Error] Failed to download %s: %v", urlStr, err), false)
        atomic.AddInt64(&j.stats.Failed, 1)
        return
    }

    // –•–µ—à–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã, –∫–∞–∫ –º—ã –∏ –¥–æ–≥–æ–≤–∞—Ä–∏–≤–∞–ª–∏—Å—å, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É /ru/assets/
    hash := ContentHash(content)

    meta := FileMetadata{
        URL:         urlStr,
        ContentType: contentType,
        Hash:        hash,
        Depth:       depth,
    }

    modifiedContent := content
    for _, handler := range j.sortedHandlers() {
        modified, err := handler.Handle(modifiedContent, meta)
        if err != nil {
            log.Printf("Handler error for %s: %v", urlStr, err)
        } else {
            modifiedContent = modified
        }
    }

    // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    _, err = SaveFileV2(j.Config.OutputDir, urlStr, modifiedContent, contentType)
    if err != nil {
        j.sendLog(fmt.Sprintf("[Error] Save failed for %s: %v", urlStr, err), false)
        atomic.AddInt64(&j.stats.Failed, 1)
        return
    }

    atomic.AddInt64(&j.stats.TotalFiles, 1)
    atomic.AddInt64(&j.stats.DownloadedBytes, int64(len(content)))
    j.sendLog(fmt.Sprintf("[Done] Saved: %s", urlStr), false)

    if depth < j.Config.MaxDepth {
        j.parseAndQueueLinks(content, contentType, urlStr, depth)
    }
}

func (j *Job) parseAndQueueLinks(content []byte, contentType, baseURL string, depth int) {
    for _, parser := range j.Parsers {
        if parser.CanParse(contentType) {
            rawLinks, err := parser.Parse(content, baseURL)
            if err != nil {
                log.Printf("Parse error for %s: %v", baseURL, err)
                continue
            }

            log.Printf("Found %d raw links in %s", len(rawLinks), baseURL)

            for _, rawLink := range rawLinks {
                normalized, err := NormalizeURL(rawLink)
                if err != nil {
                    continue
                }

                // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                if !j.Filter.ShouldDownload(normalized) {
                    // –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:
                    // reason := j.Filter.FilterReason(normalized)
                    // log.Printf("Filtered out: %s (%s)", normalized, reason)
                    continue
                }

                j.mu.Lock()
                if !j.visited[normalized] {
                    j.visited[normalized] = true
                    j.depths[normalized] = depth + 1

                    // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –î–û —Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏
                    j.activeWG.Add(1)
                    j.mu.Unlock()

                    // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å. –ï—Å–ª–∏ –∫–∞–Ω–∞–ª –ø–æ–ª–æ–Ω ‚Äî –∂–¥–µ–º.
                    select {
                    case j.pending <- normalized:
                        // –£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ
                    case <-j.ctx.Done():
                        // –ï—Å–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è, –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
                        j.activeWG.Done()
                        return
                    }
                } else {
                    j.mu.Unlock()
                }
            }
            break // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–∞—Ä—Å–µ—Ä
        }
    }
}

func (j *Job) sortedHandlers() []ContentHandler {
	handlers := make([]ContentHandler, len(j.Handlers))
	copy(handlers, j.Handlers)
	sort.Slice(handlers, func(i, k int) bool {
		return handlers[i].Priority() < handlers[k].Priority()
	})
	return handlers
}

func (j *Job) saveState() error {
	j.mu.Lock()
	defer j.mu.Unlock()

	// –°–ª–∏–≤–∞–µ–º –æ—á–µ—Ä–µ–¥—å –≤ —Å—Ä–µ–∑
	var pendingURLs []string
	for {
		select {
		case url := <-j.pending:
			pendingURLs = append(pendingURLs, url)
		default:
			// –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–∞–Ω–∞–ª –ø–æ—Å–ª–µ —Å–ª–∏–≤–∞
			j.pending = make(chan string, 5000)
			for _, url := range pendingURLs {
				j.pending <- url
			}

			// –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
			state := JobState{
				ID:          j.ID,
				RootURL:     j.RootURL,
				PendingURLs: pendingURLs,
				DepthMap:    j.depths,
				Stats:       j.stats,
				Config:      j.Config,
			}

			// –ò—Å–ø–æ–ª—å–∑—É–µ–º Marshal –≤–º–µ—Å—Ç–æ MarshalIndent –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –∏ –º–µ—Å—Ç–∞
			data, err := json.Marshal(state)
			if err != nil {
				return err
			}

			return ioutil.WriteFile(j.stateFile, data, 0644)
		}
	}
}

func (j *Job) loadState() error {
	data, err := ioutil.ReadFile(j.stateFile)
	if err != nil {
		return err
	}

	var state JobState
	if err := json.Unmarshal(data, &state); err != nil {
		return err
	}

	j.ID = state.ID
	j.RootURL = state.RootURL
	j.stats = state.Stats
	j.Config = state.Config

	j.mu.Lock()
	defer j.mu.Unlock()

	// –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –∏ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ URL
	j.depths = make(map[string]int)
	j.visited = make(map[string]bool)
	j.hashes = make(map[string]bool)

	for url, depth := range state.DepthMap {
		j.depths[url] = depth
		j.visited[url] = true
	}

	// –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—á–µ—Ä–µ–¥—å
	j.pending = make(chan string, 5000)
	for _, url := range state.PendingURLs {
		j.pending <- url
		j.activeWG.Add(1) // –î–æ–±–∞–≤–ª—è–µ–º –≤ activeWG –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ URL
	}

	// –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –∏ –ø–∞—Ä—Å–µ—Ä—ã
	parsed, _ := url.Parse(j.RootURL)
	j.Filter = &DefaultURLFilter{
		domain:   parsed.Host,
		basePath: parsed.Path,
	}
	j.BasePath = parsed.Path

	// –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º LinkRewriterHandlerV2 –≤–º–µ—Å—Ç–æ LinkRewriterHandler
	j.Handlers = []ContentHandler{&LinkRewriterHandlerV2{
		outputDir: j.Config.OutputDir,
		analyzer:  NewStrategyAnalyzer(),
	}}
	j.Parsers = []ContentParser{&HTMLParser{}, &CSSParser{}}

	return nil
}

func drainChannel(ch chan string) []string {
	var urls []string
	for {
		select {
		case url := <-ch:
			urls = append(urls, url)
		default:
			return urls
		}
	}
}

// CLI –∫–æ–º–∞–Ω–¥—ã
var rootCmd = &cobra.Command{
	Use:   "downloader",
	Short: "Website Downloader with .php to .html conversion",
}

var downloadCmd = &cobra.Command{
	Use:   "download <url>",
	Short: "Download a website",
	Args:  cobra.ExactArgs(1),
	Run: func(cmd *cobra.Command, args []string) {
		cfg := loadConfig()

		// –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
		if err := os.MkdirAll(cfg.OutputDir, 0755); err != nil {
			log.Fatalf("Failed to create output directory: %v", err)
		}

		job, err := NewJob(args[0], cfg)
		if err != nil {
			log.Fatalf("Failed to create job: %v", err)
		}

		job.Run()
	},
}

var resumeCmd = &cobra.Command{
	Use:   "resume <job-id>",
	Short: "Resume a previous download job",
	Args:  cobra.ExactArgs(1),
	Run: func(cmd *cobra.Command, args []string) {
		cfg := loadConfig()

		job := &Job{
			ID:        args[0],
			Config:    cfg,
			stateFile: filepath.Join(cfg.OutputDir, args[0]+StateFileExtension),
		}

		if err := job.loadState(); err != nil {
			log.Fatalf("Failed to load job state: %v", err)
		}

		// –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∫–∞–Ω–∞–ª—ã
		job.ctx, job.cancel = context.WithCancel(context.Background())
		job.shutdownChan = make(chan os.Signal, 1)

		// –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
		job.Downloader = NewDownloader(cfg)

		// –î–û–ë–ê–í–¨–¢–ï: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
		job.Handlers = []ContentHandler{&LinkRewriterHandlerV2{
			outputDir: cfg.OutputDir,
			analyzer:  NewStrategyAnalyzer(),
		}}

		log.Printf("Resuming job %s for %s", job.ID, job.RootURL)
		job.Run()
	},
}

func loadConfig() Config {
	// –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
	viper.SetDefault("workers", DefaultWorkers)
	viper.SetDefault("max_depth", DefaultMaxDepth)
	viper.SetDefault("retries", DefaultRetries)
	viper.SetDefault("delay", DefaultDelay)
	viper.SetDefault("max_file_size", DefaultMaxFileSize)
	viper.SetDefault("output_dir", "./downloads")
	viper.SetDefault("user_agent", DefaultUserAgent)

	// –ß—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
	viper.SetConfigName("config")
	viper.SetConfigType("yaml")
	viper.AddConfigPath(".")
	viper.ReadInConfig() // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç

	return Config{
		Workers:     viper.GetInt("workers"),
		MaxDepth:    viper.GetInt("max_depth"),
		Retries:     viper.GetInt("retries"),
		Delay:       viper.GetDuration("delay"),
		MaxFileSize: viper.GetInt64("max_file_size"),
		OutputDir:   viper.GetString("output_dir"),
		UserAgent:   viper.GetString("user_agent"),
	}
}

func init() {
	// –§–ª–∞–≥–∏ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã download
	downloadCmd.Flags().Int("workers", DefaultWorkers, "Number of concurrent workers")
	downloadCmd.Flags().Int("max-depth", DefaultMaxDepth, "Maximum recursion depth")
	downloadCmd.Flags().Int("retries", DefaultRetries, "Retry attempts per URL")
	downloadCmd.Flags().Duration("delay", DefaultDelay, "Delay between requests")
	downloadCmd.Flags().Int64("max-file-size", DefaultMaxFileSize, "Maximum file size in bytes")
	downloadCmd.Flags().String("output-dir", "./downloads", "Output directory")
	downloadCmd.Flags().String("user-agent", DefaultUserAgent, "HTTP User-Agent header")

	// –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ–ª–∞–≥–æ–≤ –∫ viper
	viper.BindPFlags(downloadCmd.Flags())

	// –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
	rootCmd.AddCommand(downloadCmd, resumeCmd)
}

func main() {
	if err := rootCmd.Execute(); err != nil {
		log.Fatal(err)
	}
}
